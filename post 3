<!-- wp:paragraph -->
<p>Cosplay has grown from its origins at fan conventions into a billion-dollar global dress phenomenon. To facilitate the imagination and reinterpretation of animated images as real garments, this paper presents an automatic costume-image generation method based on image-to-image translation. Cosplay items can be significantly diverse in their styles and shapes, and conventional methods cannot be directly applied to the wide variety of clothing images that are the focus of this study. To solve this problem, our method starts by collecting and preprocessing web images to prepare a cleaned, paired dataset of the anime and real domains. Then, we present a novel architecture for generative adversarial networks (GANs) to facilitate high-quality cosplay image generation. Our GAN consists of several effective techniques to bridge the two domains and improve both the global and local consistency of generated images. Experiments demonstrated that, with quantitative evaluation metrics, the proposed GAN performs better and produces more realistic images than conventional methods. Our codes and pretrained model are available on the web.</p>
<!-- /wp:paragraph -->

<!-- wp:heading -->
<h2 id="Sec1">Introduction</h2>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>Costume play (cosplay) is a performance art in which people wear clothes to represent specific fictional characters from their favorite sources, such as&nbsp;<em>manga</em>&nbsp;(Japanese comics) and&nbsp;<em>anime</em>&nbsp;(cartoon animation). The popularity of cosplay has spanned the globe; for example, the World Cosplay Summit, which is an annual international cosplay event, attracted approximately 300,000 people from 40 countries in 2019 [<a href="https://link.springer.com/article/10.1007/s11042-022-12576-x#ref-CR38">38</a>]. There are also astonishingly many domestic and regional cosplay contests and conventions involving diverse creative activities. To succeed at these events, it is crucial for cosplayers to wear attractive, unique, expressive cosplay looks. However, designing elaborate cosplay clothes requires imagining and reinterpreting animated images as real garments. This motivated us to devise a new system for supporting costume creation, which we call&nbsp;<em>automatic costume image generation</em>.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>As shown in Fig.&nbsp;<a href="https://link.springer.com/article/10.1007/s11042-022-12576-x#Fig1">1</a>, our aim is to generate cosplay clothing item images from anime images. This task is a subtopic of&nbsp;<em>image-to-image translation</em>, which learns a mapping that can convert an image from a source domain to a target domain. Image-to-image translation has attracted much research attention, for which several generative adversarial networks (GANs) [<a href="https://link.springer.com/article/10.1007/s11042-022-12576-x#ref-CR7">7</a>] have been presented. In the literature of fashion image translation, Yoo et al. [<a href="https://link.springer.com/article/10.1007/s11042-022-12576-x#ref-CR41">41</a>] trained a network that converted an image of a dressed personâ€™s clothing to a fashion product image using multiple discriminators. For the same task, Kwon et al. [<a href="https://link.springer.com/article/10.1007/s11042-022-12576-x#ref-CR16">16</a>] introduced a coarse-to-fine scheme to reduce the visual artifacts produced by a GAN. These conventional methods were trained using a dataset of images collected from online fashion shopping malls. Specifically, the images were manually associated so that each pair consisted of a clothing product image and its fashion model correlate. This dataset is high-quality and less diverse because all the product images were taken in the same position and the dressed person usually stood up straight. However, our task of cosplay clothing synthesis presents a different hurdle to preparing such training images: the images are not be in consistent positions and cosplay items vary wildly in style and shape (e.g., dresses, kimonos, suits, sportswear, and school uniforms), so conventional methods cannot be directly applied.</p>
<!-- /wp:paragraph -->

<!-- wp:image {"linkDestination":"custom"} -->
<figure class="wp-block-image"><a href="https://link.springer.com/article/10.1007/s11042-022-12576-x/figures/1"><img src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11042-022-12576-x/MediaObjects/11042_2022_12576_Fig1_HTML.png" alt="figure 1"/></a><figcaption><strong>Fig. 1</strong></figcaption></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>To solve this problem, this paper proposes a novel approach to cleaning a paired dataset constructed for the specific task of cosplay costume generation. In addition, we present a novel GAN for translating anime character images to clothing images. Our GAN architecture uses pix2pix [<a href="https://link.springer.com/article/10.1007/s11042-022-12576-x#ref-CR11">11</a>] as the baseline model, to which we introduce additional discriminators and losses that have shown effectiveness in conventional work [<a href="https://link.springer.com/article/10.1007/s11042-022-12576-x#ref-CR37">37</a>,&nbsp;<a href="https://link.springer.com/article/10.1007/s11042-022-12576-x#ref-CR41">41</a>]. To improve the quality of image generation, we present a novel loss, which we call input consistency loss, to fill the gap between the source domain (i.e., anime images) and the target domain (i.e., real clothing images). The results of experiments conducted using 35,633 image pairs demonstrated that our method performed better than the conventional methods in terms of three quantitative evaluation measures. Our new system will be useful not only for facilitating cosplay costume creation but also for other tasks, such as illustration-based fashion image retrieval. Figure&nbsp;<a href="https://link.springer.com/article/10.1007/s11042-022-12576-x#Fig2">2</a>&nbsp;shows examples of the results produced by the proposed method. Our codes and the pretrained model that produced these images are available on the web.<sup><a href="https://link.springer.com/article/10.1007/s11042-022-12576-x#Fn1"></a></sup></p>
<!-- /wp:paragraph -->
